{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c35e295-6d98-471c-bcd2-85ef68a1690e",
   "metadata": {},
   "source": [
    "# Progressive Growing GAN (PGGAN) with PyTorch\n",
    "\n",
    "## Overview\n",
    "\n",
    "This Notebook contains the PyTorch implementation of a Progressive Growing Generative Adversarial Network (PGGAN). PGGAN is a cutting-edge GAN architecture designed to generate high-quality images with intricate details.\n",
    "\n",
    "## What is PGGAN\n",
    "\n",
    "High-quality image production was greatly aided by the introduction of Progressive Growing GANs (PGGANs), a unique training approach for Generative Adversarial Networks (GANs). PGGANs, which were introduced in 2017 by Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen, tackle issues related to training stability, scalability, and the production of varied and intricate visuals. The main characteristic is the gradual growth of the discriminator and generator networks, which begin at low resolutions and add layers one at a time to handle higher resolutions.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Progressive Growth:** Incremental addition of layers to G and D during training for high-quality image generation.\n",
    "- **Stability:** Addresses common issues such as mode collapse and training instability for more reliable training.\n",
    "- **Quality and Variation:** The progressive training approach leads to improved image quality and the generation of diverse samples.\n",
    "\n",
    "## Visualization\n",
    "\n",
    "![Progressive Growing GAN](87937a70-cafc-43e3-a4ea-7ae472b0fef8.png)\n",
    "\n",
    "The image above visualizes the process of gradual growth. The process begins with both the generator (G) and discriminator (D) having a low spatial resolution of 4Ã—4 pixels. As training progresses, layers are incrementally added to G and D, resulting in a gradual increase in the spatial resolution of the generated images. Importantly, all existing layers remain trainable throughout this process.\n",
    "\n",
    "## Refer to Original Paper\n",
    "\n",
    "For a detailed understanding of Progressive Growing GANs, please refer to the [original paper](https://arxiv.org/pdf/1710.10196.pdf):\n",
    "\n",
    "- [Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://arxiv.org/pdf/1710.10196.pdf) by Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen (2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af50b6",
   "metadata": {
    "papermill": {
     "duration": 0.03031,
     "end_time": "2022-02-14T09:11:43.848651",
     "exception": false,
     "start_time": "2022-02-14T09:11:43.818341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Model Structure**\n",
    "![Model Structure](025262ee-87dd-4f8a-aaf6-5b8b5ed73086.png)\n",
    "\n",
    "In the image above, We can see the model structure proposed in the paper. and here in this part, we will implement the structure.\n",
    "### Tools for Building Generator:\n",
    "\n",
    "#### 1. **Equalized Learning Rate Conv2d (`WSConv2d`):**\n",
    "   - **Purpose:** Weight scaling is applied to the convolutional layers to ensure equalized learning rates, as suggested by the ProGan paper.\n",
    "   - **Why Use:** Normalizing the weights helps stabilize the learning process and improves convergence during training. It ensures that the scale of weights does not hinder the optimization.\n",
    "\n",
    "#### 2. **PixelNorm (`PixelNorm`):**\n",
    "   - **Purpose:** Normalizes individual pixel values in the input tensor, ensuring consistent scales across pixels.\n",
    "   - **Why Use:** Pixel-wise normalization helps maintain stability and improves the quality of generated images. It prevents certain pixels from dominating the learning process.\n",
    "\n",
    "#### 3. **Convolution Block (`ConvBlock`):**\n",
    "   - **Purpose:** Combines multiple layers, including equalized learning rate convolution, Leaky ReLU activation, and optional pixel-wise normalization.\n",
    "   - **Why Use:** The convolution block serves as a fundamental building block for both the Generator and Discriminator. It introduces non-linearity through activation functions and enables the network to learn hierarchical features.\n",
    "\n",
    "---\n",
    "\n",
    "### Tools for Building Discriminator:\n",
    "\n",
    "#### 1. **Equalized Learning Rate Conv2d (`WSConv2d`):**\n",
    "   - **Purpose:** Same as in the Generator, weight scaling is applied to convolutional layers for equalized learning rates.\n",
    "   - **Why Use:** Consistency in weight scaling between the Generator and Discriminator ensures stable adversarial training.\n",
    "\n",
    "#### 2. **PixelNorm (`PixelNorm`):**\n",
    "   - **Purpose:** Used for normalizing pixel values, similar to its role in the Generator.\n",
    "   - **Why Use:** Ensures that input images to the Discriminator have consistent pixel-wise scales, contributing to a stable learning process.\n",
    "\n",
    "#### 3. **Convolution Block (`ConvBlock`):**\n",
    "   - **Purpose:** Similar to the Generator, the convolution block is utilized in the Discriminator to introduce non-linearity and hierarchical feature learning.\n",
    "   - **Why Use:** Leaky ReLU activation and optional pixel-wise normalization contribute to the Discriminator's ability to discern features in the input images.\n",
    "\n",
    "#### 4. **Average Pooling (`nn.AvgPool2d`):**\n",
    "   - **Purpose:** Down-samples the input tensor using average pooling.\n",
    "   - **Why Use:** Down-sampling is crucial for progressive growing. It reduces the spatial resolution of the input, enabling the Discriminator to focus on larger-scale features in higher-resolution images.\n",
    "\n",
    "#### 5. **Minibatch Standard Deviation:**\n",
    "   - **Purpose:** Computes the standard deviation across the batch and concatenates it with the input tensor.\n",
    "   - **Why Use:** Enhances the Discriminator's ability to detect variations and patterns across the entire batch, making it less sensitive to minor differences between generated and real images.\n",
    "\n",
    "#### 6. **Fade-in Operation:**\n",
    "   - **Purpose:** Linearly interpolates between two input tensors (e.g., between down-scaled and original images).\n",
    "   - **Why Use:** Facilitates the smooth transition between resolutions during progressive growing, preventing sudden jumps and promoting stable training.\n",
    "\n",
    "These tools collectively contribute to the stability, convergence, and overall performance of both the Generator and Discriminator in the Progressive Growing GAN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b849636a",
   "metadata": {
    "papermill": {
     "duration": 1.302182,
     "end_time": "2022-02-14T09:11:45.303232",
     "exception": false,
     "start_time": "2022-02-14T09:11:44.001050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import log2\n",
    "\n",
    "# In generator conv blocks, the channels go like \"512->512->512->512->256->128->64->32->16\"\n",
    "factors = [1,1,1,1,1/2,1/4,1/8,1/16,1/32]\n",
    "\n",
    "class WSConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2):\n",
    "        \"\"\"\n",
    "        Weight-scaled convolutional layer with equalized learning rates.\n",
    "\n",
    "        Parameters:\n",
    "        - in_channels: Number of input channels.\n",
    "        - out_channels: Number of output channels.\n",
    "        - kernel_size: Size of the convolutional kernel (default: 3).\n",
    "        - stride: Stride of the convolution (default: 1).\n",
    "        - padding: Padding of the convolution (default: 1).\n",
    "        - gain: Gain for weight scaling (default: 2).\n",
    "        \"\"\"\n",
    "        super(WSConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        # Initialize conv layer\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies the convolution operation with weight scaling and adds bias.\"\"\"\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n",
    "\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Pixel-wise normalization to ensure consistent scales across pixels.\"\"\"\n",
    "        super(PixelNorm, self).__init__()\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Normalizes the input tensor by dividing each pixel by the square root of the mean of squared pixel values.\"\"\"\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n",
    "        \"\"\"\n",
    "        Convolutional block with Leaky ReLU activation.\n",
    "\n",
    "        Parameters:\n",
    "        - in_channels: Number of input channels.\n",
    "        - out_channels: Number of output channels.\n",
    "        - use_pixelnorm: Boolean indicating whether to use pixel-wise normalization (default: True).\n",
    "        \"\"\"\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.use_pn = use_pixelnorm\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixelNorm()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies Leaky ReLU activation after each convolutional layer and optionally applies pixel-wise normalization.\"\"\"\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c79844",
   "metadata": {
    "papermill": {
     "duration": 0.030664,
     "end_time": "2022-02-14T09:11:45.364589",
     "exception": false,
     "start_time": "2022-02-14T09:11:45.333925",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generator in Progressive Growing GAN\r\n",
    "\r\n",
    "The Generator in a Progressive Growing GAN is a neural network responsible for creating realistic images. Its primary function is to generate images by transforming a random noise vector into increasingly detailed and high-resolution images. Here's how it achieves this:\r\n",
    "\r\n",
    "## 1. Initial Block:\r\n",
    "- The process starts with an initial block that takes a random noise vector as input.\r\n",
    "- This block consists of operations like normalization (PixelNorm), a transposed convolution, and activation functions (LeakyReLU).\r\n",
    "- It aims to create a preliminary feature map representing basic patterns.\r\n",
    "\r\n",
    "## 2. Progressive Growing:\r\n",
    "- The Generator progressively refines the feature map through a series of blocks.\r\n",
    "- Each block includes equalized learning rate convolutions, activation functions (LeakyReLU), and optional normalization (PixelNorm).\r\n",
    "- As the training progresses, additional blocks are added, increasing the spatial resolution of the feature map.\r\n",
    "\r\n",
    "## 3. RGB Conversion:\r\n",
    "- At each step, a corresponding 1x1 convolutional layer (RGB layer) converts the feature map to the desired number of image channels (commonly 3 for RGB).\r\n",
    "- These RGB layers contribute to the final synthesized image.\r\n",
    "\r\n",
    "## 4. Fade-in Operation:\r\n",
    "- To ensure a smooth transition between different resolutions, a fade-in operation is employed.\r\n",
    "- It linearly blends the output of the current resolution with the upscaled version from the previous step.\r\n",
    "- This technique prevents abrupt changes and aids in stable training.\r\n",
    "\r\n",
    "## 5. Linear Interpolation:\r\n",
    "- Linear interpolation is used to smoothly transition between different resolutions during progressive growing.\r\n",
    "- It involves blending the images produced at different resolutions, controlled by a linear interpolation factor.\r\n",
    "\r\n",
    "## 6. Final Output:\r\n",
    "- The final output of the Generator is a high-resolution, realistic image that has been progressively refined through multiple steps.\r\n",
    "\r\n",
    "In summary, the Generator's role is to transform random noise into detailed and coherent images, progressively improving their quality during training. The fade-in operation and linear interpolation contribute to the stable evolution of generated images in the Progressive Growing GAN.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f73b38",
   "metadata": {
    "papermill": {
     "duration": 0.045862,
     "end_time": "2022-02-14T09:11:45.441222",
     "exception": false,
     "start_time": "2022-02-14T09:11:45.395360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
    "        \"\"\"\n",
    "        Generator network for Progressive Growing GAN.\n",
    "\n",
    "        Parameters:\n",
    "        - z_dim: Dimension of the input noise vector.\n",
    "        - in_channels: Number of channels in the initial convolutional layer.\n",
    "        - img_channels: Number of channels in the generated images (default: 3 for RGB).\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        # Initial structure of Generator should be opposite of ending structure of Discriminator\n",
    "        # Initial takes 1x1 -> 4x4\n",
    "        self.initial = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm(),\n",
    "        )\n",
    "\n",
    "        self.initial_rgb = WSConv2d(\n",
    "            in_channels, img_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.prog_blocks, self.rgb_layers = (\n",
    "            nn.ModuleList([]),\n",
    "            nn.ModuleList([self.initial_rgb]),\n",
    "        )\n",
    "\n",
    "        for i in range(len(factors) - 1):\n",
    "            conv_in_c = int(in_channels * factors[i])\n",
    "            conv_out_c = int(in_channels * factors[i + 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        \"\"\"Fade-in between upscaled and generated images.\"\"\"\n",
    "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        \"\"\"\n",
    "        Forward pass of the Generator.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input noise vector.\n",
    "        - alpha: Linear interpolation factor for fade-in.\n",
    "        - steps: Number of progressive growing steps.\n",
    "\n",
    "        Returns:\n",
    "        - Generated image.\n",
    "        \"\"\"\n",
    "        out = self.initial(x)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(out)\n",
    "\n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n",
    "            out = self.prog_blocks[step](upscaled)\n",
    "\n",
    "        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a73a9c",
   "metadata": {
    "papermill": {
     "duration": 0.03054,
     "end_time": "2022-02-14T09:11:45.503023",
     "exception": false,
     "start_time": "2022-02-14T09:11:45.472483",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Discriminator in Progressive Growing GAN\r\n",
    "\r\n",
    "The Discriminator in a Progressive Growing GAN is a neural network tasked with distinguishing between real and generated images. Its design allows it to handle images at different resolutions progressively. Let's explore its functionality:\r\n",
    "\r\n",
    "## 1. Progressive Discrimination:\r\n",
    "- The Discriminator works in a progressive manner, just like the Generator. It processes images at various resolutions, enabling the model to handle different levels of details.\r\n",
    "\r\n",
    "## 2. Reverse Architecture:\r\n",
    "- The Discriminator's architecture is designed in reverse compared to the Generator. It starts with higher-resolution processing and gradually decreases resolution.\r\n",
    "\r\n",
    "## 3. From-RGB Layers:\r\n",
    "- The Discriminator begins with a set of 'from-RGB' layers, each corresponding to a specific input resolution. These layers convert the input image to a feature map that the Discriminator can process.\r\n",
    "\r\n",
    "## 4. Down-Sampling:\r\n",
    "- To accommodate different resolutions, the Discriminator utilizes down-sampling through operations like Average Pooling. This downscales the input feature map while maintaining important information.\r\n",
    "\r\n",
    "## 5. Minibatch Standard Deviation:\r\n",
    "- The Discriminator incorporates a Minibatch Standard Deviation operation. This operation adds information about the variation within a batch, helping the Discriminator consider the diversity of the input images.\r\n",
    "\r\n",
    "## 6. Final Block:\r\n",
    "- The Discriminator concludes with a final block responsible for making the ultimate discrimination between real and generated images. This block includes convolutions, activation functions (LeakyReLU), and a final convolutional layer to produce the output.\r\n",
    "\r\n",
    "## 7. Fade-in Operation:\r\n",
    "- Similar to the Generator, the Discriminator employs a fade-in operation. This smoothens the transition between different resolutions, contributing to more stable training.\r\n",
    "\r\n",
    "## 8. Linear Interpolation:\r\n",
    "- Linear interpolation is used for a seamless transition in the Discriminator's evaluation of images at different resolutions. It ensures a consistent discrimination process during progressive growing.\r\n",
    "\r\n",
    "In summary, the Discriminator's role is to distinguish between real and generated images, progressively adapting to varying levels of detail. The use of from-RGB layers, down-sampling, and the fade-in operation allows the Discriminator to effectively handle images at different resolutions in the Progressive Growing GAN.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52b9fa",
   "metadata": {
    "papermill": {
     "duration": 0.050258,
     "end_time": "2022-02-14T09:11:45.583583",
     "exception": false,
     "start_time": "2022-02-14T09:11:45.533325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels, img_channels=3):\n",
    "        \"\"\"\n",
    "        Discriminator network for Progressive Growing GAN.\n",
    "\n",
    "        Parameters:\n",
    "        - in_channels: Number of channels in the initial convolutional layer.\n",
    "        - img_channels: Number of channels in the input images (default: 3 for RGB).\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        for i in range(len(factors) - 1, 0, -1):\n",
    "            conv_in = int(in_channels * factors[i])\n",
    "            conv_out = int(in_channels * factors[i - 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in, conv_out, use_pixelnorm=False))\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "        self.initial_rgb = WSConv2d(\n",
    "            img_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.final_block = nn.Sequential(\n",
    "            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, 1, kernel_size=1, padding=0, stride=1),\n",
    "        )\n",
    "\n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "        \"\"\"Fade-in between downscaled and output images.\"\"\"\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "\n",
    "    def minibatch_std(self, x):\n",
    "        \"\"\"Calculate minibatch standard deviation and concatenate it with the input.\"\"\"\n",
    "        batch_statistics = torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        return torch.cat([x, batch_statistics], dim=1)\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        \"\"\"\n",
    "        Forward pass of the Discriminator.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input image.\n",
    "        - alpha: Linear interpolation factor for fade-in.\n",
    "        - steps: Number of progressive growing steps.\n",
    "\n",
    "        Returns:\n",
    "        - Discriminator output.\n",
    "        \"\"\"\n",
    "        cur_step = len(self.prog_blocks) - steps\n",
    "\n",
    "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
    "\n",
    "        if steps == 0:\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "\n",
    "        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        for step in range(cur_step + 1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372359a5",
   "metadata": {
    "papermill": {
     "duration": 0.031495,
     "end_time": "2022-02-14T09:11:46.227220",
     "exception": false,
     "start_time": "2022-02-14T09:11:46.195725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configurations & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c474bc6c",
   "metadata": {
    "papermill": {
     "duration": 3.037618,
     "end_time": "2022-02-14T09:11:49.296216",
     "exception": false,
     "start_time": "2022-02-14T09:11:46.258598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "START_TRAIN_IMG_SIZE = 16\n",
    "DATASET = \"../input/human-faces\"\n",
    "\n",
    "CHECKPOINT_GEN = \"generator.pth\"\n",
    "CHECKPOINT_CRITIC = \"critic.pth\"\n",
    "SAVE_MODEL = False\n",
    "LOAD_MODEL = False\n",
    "\n",
    "LR = 1e-3\n",
    "BATCH_SIZES = [32,32,32,32,16,16,16,4,4,4] ## modifiable/ Batch_sizes for each step\n",
    "IMAGE_SIZE = 128 ## 1024 for paper\n",
    "IMG_CHANNELS = 3\n",
    "Z_DIM = 256 ## 512 for paper\n",
    "IN_CHANNELS = 256 ## 512 for paper\n",
    "LAMBDA_GP = 10\n",
    "NUM_STEPS = int(log2(IMAGE_SIZE/4)) + 1\n",
    "\n",
    "PROGRESSIVE_EPOCHS = [4] * len(BATCH_SIZES)\n",
    "FIXED_NOISE = torch.randn(8,Z_DIM,1,1).to(DEVICE)\n",
    "# NUM_WORKERS = 4\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd434e",
   "metadata": {
    "papermill": {
     "duration": 0.032226,
     "end_time": "2022-02-14T09:11:49.360477",
     "exception": false,
     "start_time": "2022-02-14T09:11:49.328251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1410e00",
   "metadata": {
    "papermill": {
     "duration": 0.317986,
     "end_time": "2022-02-14T09:11:49.710004",
     "exception": false,
     "start_time": "2022-02-14T09:11:49.392018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def save_on_tensorboard(writer,loss_critic,loss_gen,real,fake,tensorboard_step):\n",
    "    writer.add_scalar(\"Loss Critic\",loss_critic,global_step=tensorboard_step)\n",
    "    writer.add_scalar(\"Loss Generator\", loss_gen, global_step=tensorboard_step)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        img_grid_real = torchvision.utils.make_grid(real[:8],normalize=True)\n",
    "        img_grid_fake = torchvision.utils.make_grid(fake[:8],normalize=True)\n",
    "        \n",
    "        writer.add_image(\"Real\",img_grid_real,global_step = tensorboard_step)\n",
    "        writer.add_image(\"Fake\",img_grid_fake,global_step = tensorboard_step)\n",
    "        \n",
    "def gradient_penalty(critic,real,fake,alpha,train_step,device=\"cpu\"):\n",
    "    BATCH_SIZE,C,H,W = real.shape\n",
    "    beta = torch.rand((BATCH_SIZE,1,1,1)).repeat(1,C,H,W).to(device)\n",
    "    \n",
    "    interpolated_images = real * beta + fake.detach() * (1-beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "    \n",
    "    ## Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images,alpha,train_step)\n",
    "    \n",
    "    ## Take the gradient of the scores with respect to the image\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs = interpolated_images,\n",
    "        outputs = mixed_scores,\n",
    "        grad_outputs = torch.ones_like(mixed_scores),\n",
    "        create_graph = True,\n",
    "        retain_graph = True\n",
    "    )[0]\n",
    "    \n",
    "    gradient = gradient.view(gradient.shape[0],-1)\n",
    "    gradient_norm = gradient.norm(2,dim=1)\n",
    "    penalty = torch.mean((gradient_norm - 1)**2)\n",
    "    return penalty\n",
    "\n",
    "def save_checkpoint(model,optimizer,filename=\"my_checkpoint.pth\"):\n",
    "    print(\"Saving Checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\" : optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint,filename)\n",
    "    \n",
    "def load_checkpoint(checkpoint_file,model,optimizer,lr):\n",
    "    print(\"Loading Checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file,map_location=\"cuda\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "        \n",
    "def generate_examples(gen,current_epoch,steps,n=16):\n",
    "    gen.eval()\n",
    "    aplha = 1.0\n",
    "    \n",
    "    for i in range(n):\n",
    "        with torch.no_grad():\n",
    "            noise = torch.randn(1,Z_DIM,1,1).to(DEVICE)\n",
    "            generated_img = gen(noise,alpha=alpha,steps=steps)\n",
    "            save_image(generated_img*0.5+0.5,f\"generated_images/step{steps}_epoch{current_epoch}_{i}.png\")\n",
    "#             save_image(generated_img*0.5+0.5,f\"step:{steps}_epoch{current_epoch}_{i}.png\")\n",
    "    \n",
    "    gen.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e9bf3",
   "metadata": {
    "papermill": {
     "duration": 0.03202,
     "end_time": "2022-02-14T09:11:49.773570",
     "exception": false,
     "start_time": "2022-02-14T09:11:49.741550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce15a23b",
   "metadata": {
    "papermill": {
     "duration": 0.253317,
     "end_time": "2022-02-14T09:11:50.058686",
     "exception": false,
     "start_time": "2022-02-14T09:11:49.805369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe48283a",
   "metadata": {
    "papermill": {
     "duration": 38106.220516,
     "end_time": "2022-02-14T19:46:56.310536",
     "exception": true,
     "start_time": "2022-02-14T09:11:50.090020",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmarks = True\n",
    "\n",
    "def get_loader(img_size):\n",
    "    transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((img_size,img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.Normalize([0.5 for _ in range(IMG_CHANNELS)],[0.5 for _ in range(IMG_CHANNELS)])\n",
    "    ])\n",
    "    \n",
    "    batch_size = BATCH_SIZES[int(log2(img_size/4))]\n",
    "    dataset = datasets.ImageFolder(root=DATASET,transform=transform)\n",
    "    loader = DataLoader(dataset,batch_size=batch_size,shuffle=True,num_workers=NUM_WORKERS,pin_memory=True)\n",
    "    \n",
    "    return loader,dataset\n",
    "\n",
    "def train_fn(gen,critic,loader,dataset,step,alpha,opt_gen,opt_critic,tensorboard_step,writer,scaler_gen,scaler_critic):\n",
    "    loop = tqdm(loader,leave=True)\n",
    "    \n",
    "    i = 0\n",
    "    for batch_idx,(real,_) in enumerate(loop):\n",
    "        i += 1\n",
    "        if i%2 == 0:\n",
    "            continue\n",
    "        real = real.to(DEVICE)\n",
    "        cur_batch_size = real.shape[0]\n",
    "        noise = torch.randn(cur_batch_size,Z_DIM,1,1).to(DEVICE)\n",
    "        \n",
    "        ## Train Critic\n",
    "        ## Wasserstein Loss : Maximize \"E[Critic(real)] - E[Critic(fake)]\"   ==   Minimize \"-(E[Critic(real)] - E[Critic(fake)])\"\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake = gen(noise,alpha,step).to(DEVICE)\n",
    "            critic_real = critic(real,alpha,step)\n",
    "            critic_fake = critic(fake.detach(),alpha,step)\n",
    "            gp = gradient_penalty(critic,real,fake,alpha,step,device=DEVICE)\n",
    "            loss_critic = -1 * (torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp + 0.001 * torch.mean(critic_real**2)\n",
    "        \n",
    "        critic.zero_grad()\n",
    "        scaler_critic.scale(loss_critic).backward()\n",
    "        scaler_critic.step(opt_critic)\n",
    "        scaler_critic.update()\n",
    "        \n",
    "        ## Train Generator\n",
    "        ## Maximize \"E[Critic(fake)]\"   ==   Minimize \"- E[Critic(fake)]\"\n",
    "        with torch.cuda.amp.autocast():\n",
    "            gen_fake = critic(fake,alpha,step)\n",
    "            loss_gen = -1 * torch.mean(gen_fake)\n",
    "            \n",
    "        gen.zero_grad()\n",
    "        scaler_gen.scale(loss_gen).backward()\n",
    "        scaler_gen.step(opt_gen)\n",
    "        scaler_gen.update()\n",
    "    \n",
    "        alpha += (cur_batch_size/len(dataset)) * (1/PROGRESSIVE_EPOCHS[step]) * 2\n",
    "        alpha = min(alpha,1)\n",
    "        \n",
    "        if batch_idx % 500 == 0:\n",
    "            with torch.no_grad():\n",
    "                fixed_fakes = gen(FIXED_NOISE,alpha,step) * 0.5 + 0.5\n",
    "                save_on_tensorboard(writer,loss_critic.item(),loss_gen.item(),real.detach(),fixed_fakes.detach(),tensorboard_step)\n",
    "                tensorboard_step += 1\n",
    "    \n",
    "    return tensorboard_step,alpha\n",
    "        \n",
    "## build model\n",
    "gen = Generator(Z_DIM,IN_CHANNELS,IMG_CHANNELS).to(DEVICE)\n",
    "critic = Discriminator(IN_CHANNELS,IMG_CHANNELS).to(DEVICE)\n",
    "\n",
    "## initialize optimizer,scalers (for FP16 training)\n",
    "opt_gen = optim.Adam(gen.parameters(),lr=LR,betas=(0.0,0.99))\n",
    "opt_critic = optim.Adam(critic.parameters(),lr=LR,betas=(0.0,0.99))\n",
    "scaler_gen = torch.cuda.amp.GradScaler()\n",
    "scaler_critic = torch.cuda.amp.GradScaler()\n",
    "\n",
    "## tensorboard writer\n",
    "writer = SummaryWriter(f\"runs/PG_GAN\")\n",
    "tensorboard_step = 0\n",
    "\n",
    "## if checkpoint files exist, load model\n",
    "if LOAD_MODEL:\n",
    "    load_checkpoint(CHECKPOINT_GEN,gen,opt_gen,LR)\n",
    "    load_checkpoint(CHECKPOINT_CRITIC,critic,opt_critic,LR)\n",
    "    \n",
    "gen.train()\n",
    "critic.train()\n",
    "\n",
    "step = int(log2(START_TRAIN_IMG_SIZE/4)) ## starts from 0\n",
    "\n",
    "global_epoch = 0\n",
    "generate_examples_at = [4,8,12,16,20,24,28,32]\n",
    "\n",
    "for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n",
    "    alpha = 1e-4\n",
    "    loader,dataset = get_loader(4*2**step)\n",
    "    print(f\"Image size:{4*2**step} | Current step:{step}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Global Epoch:{global_epoch}\")\n",
    "        tensorboard_step,alpha = train_fn(gen,critic,loader,dataset,step,alpha,opt_gen,opt_critic,tensorboard_step,writer,scaler_gen,scaler_critic)\n",
    "        global_epoch += 1\n",
    "        if global_epoch in generate_examples_at:\n",
    "            generate_examples(gen,global_epoch,step,n=6)\n",
    "        \n",
    "        if SAVE_MODEL and (epoch+1)%8==0:\n",
    "            save_checkpoint(gen,opt_gen,filename=\"CHECKPOINT_GEN\")\n",
    "            save_checkpoint(critic,opt_critic,filename=\"CHECKPOINT_CRITIC\")\n",
    "            \n",
    "    step += 1 ## Progressive Growing\n",
    "    \n",
    "print(\"Training finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 38135.787549,
   "end_time": "2022-02-14T19:47:02.962091",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-14T09:11:27.174542",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
